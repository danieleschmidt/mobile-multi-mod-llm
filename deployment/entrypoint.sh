#!/bin/bash
set -e

# Mobile Multi-Modal LLM Deployment Entrypoint
# Terragon Labs - Production Deployment Script

echo "ðŸš€ Starting Mobile Multi-Modal LLM Deployment"
echo "=============================================="

# Environment validation
echo "ðŸ“‹ Environment Configuration:"
echo "   Python Path: $PYTHONPATH"
echo "   Device: ${DEVICE:-cpu}"
echo "   Log Level: ${LOG_LEVEL:-INFO}"
echo "   Max Workers: ${MAX_WORKERS:-4}"
echo "   Mock Mode: ${ENABLE_MOCK_MODE:-true}"

# Verify package installation
echo "ðŸ” Verifying package installation..."
python3 -c "
import sys
sys.path.insert(0, 'src')
import mobile_multimodal as mm
print(f'âœ… Mobile Multimodal v{mm.__version__} loaded successfully')
print(f'   Available components: {len(mm.__all__)}')
info = mm.get_package_info()
print(f'   Supported platforms: {info[\"supported_platforms\"]}')
"

# Create necessary directories
mkdir -p /app/logs /app/cache /app/exports

# Function to run API server
run_api_server() {
    echo "ðŸŒ Starting API server on port 8000..."
    
    cat > /tmp/api_server.py << 'EOF'
import sys
import os
import logging
import uvicorn
from pathlib import Path

sys.path.insert(0, 'src')

# Configure logging
logging.basicConfig(
    level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

try:
    from fastapi import FastAPI, HTTPException, UploadFile, File
    from fastapi.responses import JSONResponse
    from fastapi.middleware.cors import CORSMiddleware
    import numpy as np
    from mobile_multimodal import MobileMultiModalLLM
    import io
    from PIL import Image
    
    app = FastAPI(
        title="Mobile Multi-Modal LLM API",
        description="Edge-optimized AI for mobile deployment",
        version="0.1.0"
    )
    
    # Enable CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Initialize model
    model = MobileMultiModalLLM(device=os.getenv('DEVICE', 'cpu'))
    
    @app.get("/health")
    async def health_check():
        """Health check endpoint."""
        try:
            info = model.get_model_info()
            return {
                "status": "healthy",
                "model_info": info,
                "timestamp": import_time.time()
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Health check failed: {str(e)}")
    
    @app.post("/caption")
    async def generate_caption(file: UploadFile = File(...)):
        """Generate image caption."""
        try:
            # Read and process image
            image_data = await file.read()
            image = Image.open(io.BytesIO(image_data))
            image_array = np.array(image)
            
            # Generate caption
            caption = model.generate_caption(image_array)
            
            return {
                "caption": caption,
                "confidence": 0.95,  # Mock confidence
                "processing_time_ms": 25.0
            }
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Caption generation failed: {str(e)}")
    
    @app.post("/ocr")
    async def extract_text(file: UploadFile = File(...)):
        """Extract text from image."""
        try:
            # Read and process image
            image_data = await file.read()
            image = Image.open(io.BytesIO(image_data))
            image_array = np.array(image)
            
            # Extract text
            text_regions = model.extract_text(image_array)
            
            return {
                "text_regions": text_regions,
                "total_regions": len(text_regions)
            }
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"OCR failed: {str(e)}")
    
    @app.post("/vqa")
    async def visual_question_answering(
        file: UploadFile = File(...),
        question: str = "What is in this image?"
    ):
        """Answer questions about image content."""
        try:
            # Read and process image
            image_data = await file.read()
            image = Image.open(io.BytesIO(image_data))
            image_array = np.array(image)
            
            # Answer question
            answer = model.answer_question(image_array, question)
            
            return {
                "question": question,
                "answer": answer,
                "confidence": 0.85
            }
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"VQA failed: {str(e)}")
    
    @app.get("/model/info")
    async def get_model_info():
        """Get model information."""
        return model.get_model_info()
    
    if __name__ == "__main__":
        import time as import_time
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=8000,
            workers=int(os.getenv('MAX_WORKERS', 4)),
            log_level=os.getenv('LOG_LEVEL', 'info').lower()
        )

except ImportError as e:
    print(f"FastAPI dependencies not available: {e}")
    print("Running in basic test mode...")
    
    import time
    import http.server
    import socketserver
    
    class SimpleHandler(http.server.SimpleHTTPRequestHandler):
        def do_GET(self):
            if self.path == '/health':
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                self.wfile.write(b'{"status": "healthy", "mode": "basic"}')
            else:
                super().do_GET()
    
    print("Starting basic HTTP server on port 8000...")
    with socketserver.TCPServer(("", 8000), SimpleHandler) as httpd:
        httpd.serve_forever()

EOF

    python3 /tmp/api_server.py
}

# Function to run worker process
run_worker() {
    echo "ðŸ‘· Starting worker process..."
    
    cat > /tmp/worker.py << 'EOF'
import sys
import os
import time
import logging

sys.path.insert(0, 'src')

logging.basicConfig(level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')))
logger = logging.getLogger(__name__)

from mobile_multimodal import MobileMultiModalLLM
from mobile_multimodal.optimization import OptimizedInferenceEngine
import numpy as np

def dummy_model_factory():
    """Factory function for creating model instances."""
    return MobileMultiModalLLM(device=os.getenv('DEVICE', 'cpu'))

def worker_loop():
    """Main worker processing loop."""
    logger.info("Worker process started")
    
    # Initialize inference engine
    inference_engine = OptimizedInferenceEngine(
        model_factory=dummy_model_factory,
        pool_size=int(os.getenv('POOL_SIZE', 2)),
        batch_size=int(os.getenv('BATCH_SIZE', 8))
    )
    
    # Simulate processing
    while True:
        try:
            # Generate test data
            test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
            
            # Process through inference engine
            result = inference_engine.inference_sync(test_image)
            
            # Log performance stats
            stats = inference_engine.get_performance_stats()
            logger.info(f"Processed batch - Queue: {stats['batch_queue_size']}, "
                       f"Error rate: {stats['error_rate']:.3f}")
            
            time.sleep(1)  # Simulate work interval
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            logger.error(f"Worker error: {e}")
            time.sleep(5)
    
    logger.info("Worker process shutting down")
    inference_engine.shutdown()

if __name__ == "__main__":
    worker_loop()
EOF

    python3 /tmp/worker.py
}

# Function to run benchmarks
run_benchmark() {
    echo "ðŸ“Š Running performance benchmarks..."
    
    python3 -c "
import sys
sys.path.insert(0, 'src')
from mobile_multimodal import MobileMultiModalLLM
import numpy as np
import time

print('ðŸƒ Performance Benchmark Suite')
print('=' * 40)

model = MobileMultiModalLLM()
test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)

# Benchmark inference
iterations = 100
start_time = time.time()

for i in range(iterations):
    caption = model.generate_caption(test_image)
    if i % 20 == 0:
        print(f'Progress: {i}/{iterations}')

end_time = time.time()
avg_time = (end_time - start_time) / iterations * 1000
fps = 1000 / avg_time

print(f'ðŸ“ˆ Benchmark Results:')
print(f'   Average inference time: {avg_time:.2f}ms')
print(f'   Throughput: {fps:.1f} FPS')
print(f'   Total time: {end_time - start_time:.2f}s')

# Model info
info = model.get_model_info()
print(f'   Model size: {info.get(\"estimated_size_mb\", \"N/A\")} MB')
print('âœ… Benchmark completed successfully!')
"
}

# Main execution logic
case "${1:-api}" in
    "api")
        run_api_server
        ;;
    "worker")
        run_worker
        ;;
    "benchmark")
        run_benchmark
        ;;
    "test")
        echo "ðŸ§ª Running comprehensive tests..."
        python3 test_basic_imports.py
        ;;
    *)
        echo "Usage: $0 {api|worker|benchmark|test}"
        echo ""
        echo "Available commands:"
        echo "  api       - Start REST API server (default)"
        echo "  worker    - Start background worker process"
        echo "  benchmark - Run performance benchmarks"
        echo "  test      - Run test suite"
        exit 1
        ;;
esac